{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapted from http://mlbernauer.com/R/20160131-document-retrieval-sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import codecs\n",
    "from Bio import Medline\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ptighe/Documents/Python Projects/AcutePainOntology/medline_testing'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='pubmed_result_medline.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taken directly from reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dictionary of medline terms: https://www.nlm.nih.gov/bsd/mms/medlineelements.html\n",
    "def read_medline_data(filename):\n",
    "    recs = Medline.parse(open(filename, 'r'))\n",
    "    text = pd.DataFrame(columns = [\"pmid\", \"articletitle\", \"journaltitle\", \"authors\", \"affiliation\", \"grant\",\n",
    "                                   \"abstract\", \"pubdate\"])\n",
    "    count = 0\n",
    "    for rec in recs:\n",
    "        try:\n",
    "            abstr = rec[\"AB\"]\n",
    "            atitle = rec[\"TI\"]\n",
    "            auths = rec[\"AU\"]\n",
    "            pubdate = rec[\"DP\"]\n",
    "            jtitle = rec[\"JT\"]\n",
    "            grant=rec[\"GR\"]\n",
    "            pmid = rec['PMID']\n",
    "            affil = rec['AD']\n",
    "            text = text.append(pd.DataFrame([[pmid,atitle, jtitle, auths, affil,grant, abstr,pubdate]],\n",
    "                                     columns=[\"pmid\", \"articletitle\", \"journaltitle\", \"authors\", \"affiliation\", \n",
    "                                              \"grant\", \"abstract\", \"pubdate\"]),ignore_index=True)            \n",
    "        except:\n",
    "            pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in MEDLINE formatted text\n",
    "papers = read_medline_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BACKGROUND: Consensus indicates that a comprehensive,multimodal, holistic approach is foundational to the practice of acute pain medicine (APM),but lack of uniform, evidence-based clinical pathways leads to undesirable variability throughout U. S. healthcare systems. Acute pain studies are inconsistently synthesized to guide educational programs. Advanced practice techniques involving regional anesthesia assume the presence of a physician-led, multidisciplinary acute pain service,which is often unavailable or inconsistently applied.This heterogeneity of educational and organizational standards may result in unnecessary patient pain and escalation of healthcare costs. METHODS: A multidisciplinary panel was nominated through the APM Shared Interest Group of the American Academy of Pain Medicine. The panel met in Chicago, IL, in July 2014, to identify gaps and set priorities in APM research and education. RESULTS: The panel identified three areas of critical need: 1) an open-source acute pain data registry and clinical support tool to inform clinical decision making and resource allocation and to enhance research efforts; 2) a strong professional APM identity as an accredited subspecialty; and 3) educational goals targeted toward third-party payers,hospital administrators, and other key stake holders to convey the importance of APM. CONCLUSION: This report is the first step in a 3-year initiative aimed at creating conditions and incentives for the optimal provision of APM services to facilitate and enhance the quality of patient recovery after surgery, illness, or trauma. The ultimate goal is to reduce the conversion of acute pain to the debilitating disease of chronic pain.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.loc[0,'abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to find a list of subheadings to remove?\n",
    "\n",
    "shp = re.compile(r'^[A-Z\\d]+$')\n",
    "\n",
    "subheadings =[u\"OBJECTIVES:\", u\"OBJECTIVE:\", u\"AIMS:\", u\"OBJECTIVE/BACKGROUND:\",\n",
    "              u\"METHODS AND RESULTS:\", u\"CONCLUSIONS:\", u\"RESULTS:\", u\"METHODS:\", u\"INTRODUCTION:\", u\"BACKGROUND:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now some items for cleaning the abstract text\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist.extend(['\\x0c', '\\n'])\n",
    "subheadings =[u\"OBJECTIVES:\", u\"OBJECTIVE:\", u\"AIMS:\", u\"OBJECTIVE/BACKGROUND:\",\n",
    "              u\"METHODS AND RESULTS:\", u\"CONCLUSIONS:\", u\"RESULTS:\", u\"METHODS:\", u\"INTRODUCTION:\", u\"BACKGROUND:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers['split_abstract'] = papers['abstract'].str.split()\n",
    "papers['split_abstract'] = papers['split_abstract'].apply(lambda x: [item for item in x if item not in subheadings])\n",
    "papers['split_abstract'] = papers['split_abstract'].apply(lambda x: ' '.join(x).lower().split())\n",
    "papers['split_abstract'] = papers['split_abstract'].apply(lambda x: [item for item in x if item not in stoplist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "papers['cleaned_abstract']=papers['split_abstract'].apply(lambda x: ' '.join(x))\n",
    "papers['cleaned_abstract']=papers['cleaned_abstract'].str.replace(\"[^a-zA-Z]\",\" \").str.replace(\"\\s+\",\" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     consensus indicates comprehensive multimodal h...\n",
       "1     older adults increased risk develop frequent p...\n",
       "2     prior work postoperative pain trajectories exa...\n",
       "3     given ability process highly dimensional datas...\n",
       "4     despite widespread popularity social media lit...\n",
       "5     objective study determine effects age sex type...\n",
       "6     prior work addressed sex differences incidence...\n",
       "7     although prior work investigated interplay dem...\n",
       "8     american academy pain medicine american societ...\n",
       "9     goal project explore association post anesthes...\n",
       "10    recent years field acute pain medicine apm wit...\n",
       "11    purpose project determine whether machine lear...\n",
       "12    although million patients united states underg...\n",
       "Name: cleaned_abstract, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['cleaned_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers['split_articletitle'] = papers['articletitle'].str.split()\n",
    "papers['split_articletitle'] = papers['split_articletitle'].apply(lambda x: ' '.join(x).lower().split())\n",
    "papers['split_articletitle'] = papers['split_articletitle'].apply(lambda x: [item for item in x if item not in stoplist])\n",
    "papers['cleaned_articletitle']=papers['split_articletitle'].apply(lambda x: ' '.join(x))\n",
    "papers['cleaned_articletitle']=papers['cleaned_articletitle'].str.replace(\"[^a-zA-Z]\",\" \").str.replace(\"\\s\\s+\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      acute pain medicine united states status report \n",
       "1     age differences cytokine expression conditions...\n",
       "2     time onset sustained postoperative pain relief...\n",
       "3     teaching machine feel postoperative pain combi...\n",
       "4     painful tweet text sentiment community structu...\n",
       "5     clinically derived early postoperative pain tr...\n",
       "6     sex differences incidence severe pain events f...\n",
       "7     geospatial analysis hospital consumer assessme...\n",
       "8                   acute pain medicine anesthesiology \n",
       "9     rough starts smooth finishes correlations post...\n",
       "10              evolution practice acute pain medicine \n",
       "11    use machine learning classifiers predict reque...\n",
       "12    primary payer status associated use nerve bloc...\n",
       "Name: cleaned_articletitle, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['cleaned_articletitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [BACKGROUND, :, Consensus, indicates, that, a,...\n",
       "1     [Older, adults, are, at, an, increased, risk, ...\n",
       "2     [OBJECTIVES, :, Prior, work, on, postoperative...\n",
       "3     [BACKGROUND, :, Given, their, ability, to, pro...\n",
       "4     [BACKGROUND, :, Despite, the, widespread, popu...\n",
       "5     [The, objective, of, this, study, was, to, det...\n",
       "6     [OBJECTIVE/BACKGROUND, :, Prior, work, has, no...\n",
       "7     [Although, prior, work, has, investigated, the...\n",
       "8     [The, American, Academy, of, Pain, Medicine, a...\n",
       "9     [OBJECTIVE, :, The, goal, of, this, project, w...\n",
       "10    [BACKGROUND, :, In, recent, years, ,, the, fie...\n",
       "11    [OBJECTIVE, :, The, purpose, of, this, project...\n",
       "12    [INTRODUCTION, :, Although, more, than, 30, mi...\n",
       "Name: ra_tokens, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['ca_tokens'] =papers['cleaned_abstract'].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
    "papers['ra_tokens'] =papers['abstract'].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
    "papers['ra_tokens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11b1dd240>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUx+6QBOxxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b24d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#These give token counts\n",
    "papers['rabs_length'] =papers['ra_tokens'].map(lambda text: len(text))\n",
    "papers['cabs_length'] =papers['ca_tokens'].map(lambda text: len(text))\n",
    "papers.rabs_length.plot(bins=5, kind='hist', color='r')\n",
    "papers.cabs_length.plot(bins=5, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     137\n",
       "1      83\n",
       "2      88\n",
       "3      90\n",
       "4     101\n",
       "5      95\n",
       "6      96\n",
       "7      94\n",
       "8      68\n",
       "9      91\n",
       "10     83\n",
       "11    105\n",
       "12    109\n",
       "Name: cabs_u_length, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['cabs_u_length'] =papers['ca_tokens'].map(lambda text: len(set(text)))\n",
    "papers['cabs_u_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'three': 1, 'critical': 1, 'debilitating': 1,...\n",
       "1     {'generally': 1, 'compared': 1, 'assertion': 1...\n",
       "2     {'discussion': 1, 'event': 2, 'necessary': 1, ...\n",
       "3     {'compared': 1, 'predicting': 2, 'comparison':...\n",
       "4     {'cities': 2, 'median': 2, 'social': 4, 'compa...\n",
       "5     {'role': 1, 'interval': 1, 'biopsychosocial': ...\n",
       "6     {'compared': 1, 'nursing': 1, 'prior': 1, 'cen...\n",
       "7     {'rates': 1, 'evaluate': 1, 'healthcare': 1, '...\n",
       "8     {'deeper': 1, 'well': 1, 'include': 1, 'unders...\n",
       "9     {'median': 4, 'consistent': 1, 'reviewed': 1, ...\n",
       "10    {'nomenclature': 1, 'discussion': 1, 'role': 2...\n",
       "11    {'compared': 1, 'classifiers': 9, 'improved': ...\n",
       "12    {'interval': 1, 'freestanding': 1, 'reviewed':...\n",
       "Name: ca_tokens, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = papers['ca_tokens'].map(lambda text: nltk.FreqDist(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word counts and such using CountVectorizer from https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ca_tdm = pd.DataFrame(vectorizer.fit_transform(papers.cleaned_abstract).toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>absolute</th>\n",
       "      <th>academic</th>\n",
       "      <th>academy</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accounted</th>\n",
       "      <th>accredited</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>without</th>\n",
       "      <th>witnessed</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>younger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 796 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability  absolute  academic  academy  according  account  accounted  \\\n",
       "0         0         0         0        1          0        0          0   \n",
       "1         0         0         0        0          0        1          0   \n",
       "2         0         0         0        0          0        0          0   \n",
       "3         2         1         0        0          0        0          0   \n",
       "4         0         0         0        0          0        0          0   \n",
       "5         0         0         0        0          0        0          0   \n",
       "6         0         0         1        0          0        0          0   \n",
       "7         0         0         0        0          0        1          0   \n",
       "8         0         0         0        1          0        0          0   \n",
       "9         0         0         0        0          0        0          0   \n",
       "10        0         0         0        0          0        0          0   \n",
       "11        1         0         0        0          1        0          0   \n",
       "12        0         0         0        0          1        0          1   \n",
       "\n",
       "    accredited  accuracies  accuracy   ...     without  witnessed  word  work  \\\n",
       "0            1           0         0   ...           0          0     0     0   \n",
       "1            0           0         0   ...           0          0     0     0   \n",
       "2            0           0         0   ...           0          0     0     1   \n",
       "3            0           1         2   ...           0          0     0     0   \n",
       "4            0           0         0   ...           0          0     1     1   \n",
       "5            0           0         0   ...           0          0     0     0   \n",
       "6            0           0         0   ...           0          0     0     1   \n",
       "7            0           0         0   ...           0          0     0     1   \n",
       "8            0           0         0   ...           0          0     0     0   \n",
       "9            0           0         0   ...           0          0     0     0   \n",
       "10           0           0         0   ...           0          1     0     0   \n",
       "11           0           0         0   ...           0          0     0     0   \n",
       "12           0           0         0   ...           1          0     0     0   \n",
       "\n",
       "    would  year  years  yield  yielded  younger  \n",
       "0       0     1      0      0        0        0  \n",
       "1       0     0      0      0        0        2  \n",
       "2       0     0      0      0        0        0  \n",
       "3       0     0      0      0        0        0  \n",
       "4       0     0      0      0        0        0  \n",
       "5       0     1      0      0        0        1  \n",
       "6       0     1      0      0        1        0  \n",
       "7       0     0      0      0        0        0  \n",
       "8       0     0      0      0        0        0  \n",
       "9       0     1      0      0        0        0  \n",
       "10      0     0      1      0        0        0  \n",
       "11      1     0      0      1        1        0  \n",
       "12      0     1      0      0        0        0  \n",
       "\n",
       "[13 rows x 796 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(papers['cleaned_abstract'])\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 796)\n"
     ]
    }
   ],
   "source": [
    "print (train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'absolute', 'academic', 'academy', 'according', 'account', 'accounted', 'accredited', 'accuracies', 'accuracy', 'achieving', 'across', 'actual', 'acute', 'addition', 'additional', 'addressed', 'administration', 'administrators', 'adult', 'adults', 'advanced', 'advancement', 'adverse', 'adversely', 'affect', 'age', 'aging', 'aim', 'aimed', 'algorithm', 'algorithms', 'allocation', 'alpha', 'also', 'alternative', 'although', 'always', 'ambulatory', 'american', 'among', 'amongst', 'analgesia', 'analyses', 'analysis', 'analyze', 'analyzed', 'anatomic', 'anesthesia', 'anesthesiology', 'anesthetics', 'angeles', 'anti', 'apm', 'apple', 'application', 'applied', 'approach', 'approximately', 'aps', 'area', 'areas', 'array', 'assertion', 'assessment', 'assessments', 'associated', 'association', 'assume', 'autoregressive', 'average', 'averages', 'based', 'baseline', 'bayesian', 'begun', 'beyond', 'biopsychosocial', 'block', 'blocks', 'blood', 'boosted', 'briefly', 'but', 'ca', 'calculated', 'care', 'carry', 'case', 'cases', 'catheter', 'center', 'central', 'challenge', 'changes', 'characteristics', 'charges', 'charity', 'chicago', 'chronic', 'ci', 'circumstances', 'cities', 'classification', 'classifiers', 'classify', 'clinical', 'clinically', 'clustered', 'clustering', 'clusters', 'cochran', 'cohort', 'cold', 'collect', 'collected', 'combined', 'common', 'communication', 'communications', 'compared', 'comparison', 'comparisons', 'complementary', 'complex', 'complications', 'component', 'comprehensive', 'computational', 'conclusion', 'conditions', 'conducted', 'confidence', 'conjunction', 'connectedness', 'consensus', 'considerations', 'considered', 'considering', 'consistent', 'consistently', 'consultation', 'consultations', 'consumer', 'containment', 'content', 'context', 'continental', 'control', 'controlling', 'conversion', 'convey', 'correctly', 'correlation', 'correlations', 'cost', 'costs', 'could', 'count', 'course', 'cpt', 'creating', 'critical', 'cross', 'curve', 'cytokines', 'daily', 'data', 'dataset', 'datasets', 'day', 'days', 'debilitating', 'decision', 'decrease', 'decreased', 'decreases', 'deeper', 'define', 'defined', 'definitions', 'demarcated', 'demographic', 'demographics', 'demonstrated', 'derived', 'design', 'despite', 'determine', 'develop', 'development', 'differed', 'difference', 'differences', 'different', 'digestive', 'dimensional', 'direct', 'discuss', 'discussion', 'disease', 'diseases', 'dissemination', 'distances', 'distinct', 'distributed', 'documented', 'don', 'dysregulation', 'education', 'educational', 'effect', 'effects', 'efficiency', 'efforts', 'electronic', 'elevations', 'emerging', 'emotion', 'end', 'enhance', 'ensemble', 'ensuing', 'episodes', 'equipment', 'equivalent', 'escalation', 'essential', 'established', 'estimates', 'etc', 'evaluate', 'evaluated', 'evaluating', 'event', 'events', 'evidence', 'evolution', 'evolving', 'examine', 'examined', 'exhibited', 'expanded', 'experience', 'experienced', 'experimental', 'expertise', 'explanations', 'explore', 'extend', 'extended', 'extent', 'facilitate', 'facilities', 'factors', 'feature', 'feel', 'fellow', 'female', 'females', 'fhp', 'field', 'first', 'five', 'focal', 'focused', 'following', 'forecast', 'forecasting', 'foundational', 'freestanding', 'frequent', 'full', 'function', 'future', 'gaps', 'gender', 'generally', 'geographic', 'geographical', 'geopolitical', 'geospatial', 'geospatially', 'given', 'goal', 'goals', 'government', 'gradient', 'greater', 'group', 'groups', 'guide', 'haenszel', 'happy', 'hcahps', 'healthcare', 'healthy', 'heat', 'here', 'heterogeneity', 'heterogeneous', 'high', 'higher', 'highest', 'highly', 'historical', 'holders', 'holistic', 'hospital', 'hospitals', 'hour', 'hours', 'however', 'human', 'hundreds', 'hypothesis', 'identified', 'identify', 'identity', 'il', 'illness', 'immune', 'impact', 'implications', 'importance', 'important', 'improved', 'inability', 'incentives', 'incidence', 'include', 'included', 'including', 'inconsistently', 'increased', 'increasing', 'index', 'indicates', 'individual', 'individuals', 'indwelling', 'inflammation', 'inflammatory', 'influence', 'inform', 'initiative', 'inpatient', 'institution', 'insurance', 'integral', 'integrated', 'integumentary', 'interest', 'interindividual', 'interplay', 'interquartile', 'interval', 'intervals', 'intervention', 'intrasurvey', 'investigated', 'involved', 'involving', 'iqr', 'january', 'july', 'june', 'kendall', 'key', 'km', 'known', 'lack', 'large', 'lasso', 'lastly', 'later', 'leads', 'learning', 'least', 'led', 'length', 'less', 'level', 'likelihood', 'likelihoods', 'likely', 'limiting', 'linear', 'link', 'little', 'location', 'logistic', 'los', 'love', 'lower', 'lowest', 'machine', 'making', 'male', 'males', 'management', 'manchester', 'manila', 'mantel', 'materials', 'maximum', 'may', 'mean', 'meanings', 'measured', 'measurements', 'measures', 'mechanisms', 'media', 'median', 'medical', 'medicine', 'merely', 'merit', 'met', 'method', 'methods', 'metropolitan', 'mild', 'million', 'min', 'minimal', 'minutes', 'mixed', 'model', 'models', 'moderate', 'modern', 'moran', 'multidisciplinary', 'multimodal', 'multiple', 'musculoskeletal', 'must', 'naivebayesupdateable', 'national', 'nearest', 'necessary', 'need', 'neighbor', 'nerve', 'nervous', 'network', 'networks', 'neural', 'next', 'nn', 'nociception', 'node', 'nomenclature', 'nominated', 'non', 'nonambulatory', 'nonobstetric', 'nonzero', 'not', 'novel', 'nrs', 'number', 'numeric', 'numerical', 'nursing', 'obama', 'objective', 'observation', 'observations', 'observed', 'obstetric', 'obtained', 'occurring', 'odds', 'offer', 'often', 'older', 'one', 'online', 'open', 'operating', 'operator', 'optimal', 'optimized', 'or', 'organization', 'organizational', 'orthopedic', 'outcome', 'outcomes', 'overall', 'pacu', 'pain', 'painful', 'panel', 'party', 'pathophysiologic', 'pathways', 'patient', 'patients', 'pattern', 'patterns', 'pay', 'payers', 'payment', 'peak', 'peaked', 'pepm', 'per', 'percentage', 'perceptions', 'performance', 'performed', 'performing', 'period', 'periods', 'perioperative', 'peripheral', 'pertaining', 'phase', 'physician', 'placement', 'pod', 'pods', 'points', 'policy', 'popularity', 'population', 'positive', 'post', 'posted', 'postoperative', 'posts', 'practice', 'practices', 'pre', 'predict', 'predicted', 'predicting', 'prediction', 'predictive', 'preoperative', 'presence', 'preserved', 'pressor', 'prevalence', 'primary', 'prior', 'priorities', 'private', 'pro', 'procedure', 'procedures', 'process', 'processing', 'produced', 'professional', 'profile', 'program', 'programs', 'project', 'prolonged', 'prompt', 'proportion', 'proposes', 'providers', 'provision', 'published', 'purpose', 'quality', 'range', 'ranged', 'ranging', 'rapid', 'rates', 'rating', 'ratio', 'receive', 'receiver', 'receiving', 'recent', 'recently', 'recognition', 'recognized', 'record', 'recorded', 'records', 'recovery', 'reduce', 'reduced', 'reduction', 'reflect', 'reflecting', 'regard', 'regional', 'registry', 'regression', 'related', 'relief', 'rely', 'remains', 'report', 'reported', 'reports', 'repository', 'represent', 'represents', 'request', 'requests', 'require', 'required', 'requirements', 'requiring', 'research', 'resident', 'residents', 'resource', 'response', 'responses', 'result', 'results', 'retrospective', 'review', 'reviewed', 'risk', 'roc', 'role', 'sad', 'sample', 'satisfaction', 'scale', 'scope', 'score', 'scores', 'seasonal', 'second', 'secondary', 'seconds', 'sectional', 'selection', 'self', 'sentiment', 'sequential', 'service', 'services', 'sessions', 'set', 'setting', 'severe', 'sex', 'shared', 'show', 'shrinkage', 'significant', 'significantly', 'similar', 'simple', 'single', 'slightly', 'social', 'society', 'socioeconomic', 'solution', 'source', 'sparsity', 'spe', 'special', 'specifically', 'spes', 'staffing', 'stake', 'standards', 'states', 'statistical', 'statistically', 'statistics', 'step', 'stimuli', 'strategy', 'stratification', 'strong', 'strongly', 'structures', 'studies', 'study', 'subacute', 'subjects', 'subsequent', 'subspecialty', 'successful', 'suggest', 'summary', 'support', 'supported', 'suppr', 'surge', 'surgery', 'surgical', 'survey', 'sustained', 'symptom', 'synthesized', 'system', 'systems', 'take', 'taken', 'targeted', 'task', 'tau', 'teaching', 'teams', 'techniques', 'temporally', 'term', 'terms', 'tertiary', 'test', 'tested', 'theory', 'therapies', 'thermal', 'third', 'this', 'three', 'throughout', 'time', 'times', 'tired', 'tnf', 'together', 'tool', 'tools', 'total', 'toward', 'traditional', 'training', 'trajectories', 'trajectory', 'trauma', 'treating', 'tree', 'tweeters', 'tweets', 'twitter', 'two', 'type', 'ultimate', 'unavailable', 'unclear', 'undergo', 'undergoing', 'understanding', 'underwent', 'undesirable', 'uniform', 'uninterrupted', 'unique', 'unit', 'united', 'units', 'university', 'unnecessary', 'unobtrusively', 'use', 'used', 'users', 'using', 'utilizing', 'validated', 'variability', 'variables', 'variety', 'vector', 'versus', 'vexing', 'vital', 'vs', 'warrants', 'weighted', 'well', 'whether', 'which', 'widespread', 'within', 'without', 'witnessed', 'word', 'work', 'would', 'year', 'years', 'yield', 'yielded', 'younger']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ability\n",
      "1 absolute\n",
      "1 academic\n",
      "2 academy\n",
      "2 according\n",
      "2 account\n",
      "1 accounted\n",
      "1 accredited\n",
      "1 accuracies\n",
      "2 accuracy\n",
      "1 achieving\n",
      "3 across\n",
      "1 actual\n",
      "19 acute\n",
      "2 addition\n",
      "1 additional\n",
      "2 addressed\n",
      "1 administration\n",
      "1 administrators\n",
      "4 adult\n",
      "4 adults\n",
      "1 advanced\n",
      "1 advancement\n",
      "1 adverse\n",
      "1 adversely\n",
      "3 affect\n",
      "6 age\n",
      "1 aging\n",
      "2 aim\n",
      "1 aimed\n",
      "4 algorithm\n",
      "4 algorithms\n",
      "1 allocation\n",
      "2 alpha\n",
      "2 also\n",
      "1 alternative\n",
      "3 although\n",
      "1 always\n",
      "7 ambulatory\n",
      "3 american\n",
      "3 among\n",
      "1 amongst\n",
      "1 analgesia\n",
      "6 analyses\n",
      "4 analysis\n",
      "1 analyze\n",
      "1 analyzed\n",
      "1 anatomic\n",
      "6 anesthesia\n",
      "1 anesthesiology\n",
      "3 anesthetics\n",
      "1 angeles\n",
      "1 anti\n",
      "12 apm\n",
      "2 apple\n",
      "1 application\n",
      "1 applied\n",
      "1 approach\n",
      "1 approximately\n",
      "5 aps\n",
      "3 area\n",
      "2 areas\n",
      "1 array\n",
      "1 assertion\n",
      "2 assessment\n",
      "1 assessments\n",
      "2 associated\n",
      "4 association\n",
      "1 assume\n",
      "1 autoregressive\n",
      "1 average\n",
      "1 averages\n",
      "3 based\n",
      "2 baseline\n",
      "1 bayesian\n",
      "1 begun\n",
      "1 beyond\n",
      "2 biopsychosocial\n",
      "9 block\n",
      "1 blocks\n",
      "1 blood\n",
      "2 boosted\n",
      "1 briefly\n",
      "1 but\n",
      "1 ca\n",
      "2 calculated\n",
      "4 care\n",
      "2 carry\n",
      "2 case\n",
      "3 cases\n",
      "1 catheter\n",
      "1 center\n",
      "1 central\n",
      "1 challenge\n",
      "3 changes\n",
      "2 characteristics\n",
      "2 charges\n",
      "1 charity\n",
      "1 chicago\n",
      "1 chronic\n",
      "12 ci\n",
      "1 circumstances\n",
      "2 cities\n",
      "1 classification\n",
      "9 classifiers\n",
      "1 classify\n",
      "10 clinical\n",
      "1 clinically\n",
      "1 clustered\n",
      "1 clustering\n",
      "1 clusters\n",
      "1 cochran\n",
      "7 cohort\n",
      "1 cold\n",
      "1 collect\n",
      "4 collected\n",
      "1 combined\n",
      "2 common\n",
      "1 communication\n",
      "1 communications\n",
      "5 compared\n",
      "1 comparison\n",
      "1 comparisons\n",
      "1 complementary\n",
      "1 complex\n",
      "1 complications\n",
      "2 component\n",
      "1 comprehensive\n",
      "3 computational\n",
      "3 conclusion\n",
      "1 conditions\n",
      "1 conducted\n",
      "5 confidence\n",
      "1 conjunction\n",
      "1 connectedness\n",
      "1 consensus\n",
      "1 considerations\n",
      "1 considered\n",
      "1 considering\n",
      "3 consistent\n",
      "1 consistently\n",
      "4 consultation\n",
      "1 consultations\n",
      "1 consumer\n",
      "1 containment\n",
      "3 content\n",
      "3 context\n",
      "1 continental\n",
      "1 control\n",
      "3 controlling\n",
      "1 conversion\n",
      "1 convey\n",
      "1 correctly\n",
      "4 correlation\n",
      "3 correlations\n",
      "1 cost\n",
      "1 costs\n",
      "1 could\n",
      "2 count\n",
      "1 course\n",
      "3 cpt\n",
      "1 creating\n",
      "1 critical\n",
      "1 cross\n",
      "3 curve\n",
      "2 cytokines\n",
      "1 daily\n",
      "11 data\n",
      "1 dataset\n",
      "1 datasets\n",
      "3 day\n",
      "1 days\n",
      "1 debilitating\n",
      "3 decision\n",
      "1 decrease\n",
      "3 decreased\n",
      "1 decreases\n",
      "1 deeper\n",
      "1 define\n",
      "1 defined\n",
      "2 definitions\n",
      "1 demarcated\n",
      "2 demographic\n",
      "2 demographics\n",
      "1 demonstrated\n",
      "1 derived\n",
      "3 design\n",
      "1 despite\n",
      "3 determine\n",
      "1 develop\n",
      "3 development\n",
      "2 differed\n",
      "1 difference\n",
      "5 differences\n",
      "2 different\n",
      "1 digestive\n",
      "4 dimensional\n",
      "1 direct\n",
      "2 discuss\n",
      "3 discussion\n",
      "3 disease\n",
      "1 diseases\n",
      "1 dissemination\n",
      "1 distances\n",
      "1 distinct\n",
      "1 distributed\n",
      "2 documented\n",
      "1 don\n",
      "1 dysregulation\n",
      "3 education\n",
      "3 educational\n",
      "1 effect\n",
      "4 effects\n",
      "2 efficiency\n",
      "1 efforts\n",
      "2 electronic\n",
      "3 elevations\n",
      "1 emerging\n",
      "1 emotion\n",
      "2 end\n",
      "2 enhance\n",
      "2 ensemble\n",
      "1 ensuing\n",
      "1 episodes\n",
      "1 equipment\n",
      "1 equivalent\n",
      "1 escalation\n",
      "1 essential\n",
      "1 established\n",
      "1 estimates\n",
      "1 etc\n",
      "1 evaluate\n",
      "1 evaluated\n",
      "1 evaluating\n",
      "2 event\n",
      "2 events\n",
      "2 evidence\n",
      "3 evolution\n",
      "1 evolving\n",
      "3 examine\n",
      "4 examined\n",
      "1 exhibited\n",
      "1 expanded\n",
      "3 experience\n",
      "1 experienced\n",
      "3 experimental\n",
      "1 expertise\n",
      "1 explanations\n",
      "4 explore\n",
      "1 extend\n",
      "1 extended\n",
      "1 extent\n",
      "2 facilitate\n",
      "1 facilities\n",
      "3 factors\n",
      "2 feature\n",
      "1 feel\n",
      "1 fellow\n",
      "3 female\n",
      "2 females\n",
      "2 fhp\n",
      "2 field\n",
      "5 first\n",
      "2 five\n",
      "1 focal\n",
      "1 focused\n",
      "1 following\n",
      "2 forecast\n",
      "1 forecasting\n",
      "1 foundational\n",
      "1 freestanding\n",
      "1 frequent\n",
      "1 full\n",
      "1 function\n",
      "1 future\n",
      "1 gaps\n",
      "2 gender\n",
      "1 generally\n",
      "4 geographic\n",
      "1 geographical\n",
      "1 geopolitical\n",
      "2 geospatial\n",
      "2 geospatially\n",
      "2 given\n",
      "3 goal\n",
      "1 goals\n",
      "1 government\n",
      "2 gradient\n",
      "5 greater\n",
      "1 group\n",
      "3 groups\n",
      "1 guide\n",
      "1 haenszel\n",
      "1 happy\n",
      "4 hcahps\n",
      "3 healthcare\n",
      "1 healthy\n",
      "1 heat\n",
      "2 here\n",
      "1 heterogeneity\n",
      "1 heterogeneous\n",
      "2 high\n",
      "2 higher\n",
      "3 highest\n",
      "1 highly\n",
      "2 historical\n",
      "1 holders\n",
      "1 holistic\n",
      "6 hospital\n",
      "5 hospitals\n",
      "9 hour\n",
      "7 hours\n",
      "2 however\n",
      "1 human\n",
      "1 hundreds\n",
      "3 hypothesis\n",
      "2 identified\n",
      "1 identify\n",
      "1 identity\n",
      "8 il\n",
      "1 illness\n",
      "3 immune\n",
      "1 impact\n",
      "2 implications\n",
      "1 importance\n",
      "1 important\n",
      "1 improved\n",
      "1 inability\n",
      "1 incentives\n",
      "4 incidence\n",
      "2 include\n",
      "7 included\n",
      "1 including\n",
      "2 inconsistently\n",
      "5 increased\n",
      "1 increasing\n",
      "1 index\n",
      "1 indicates\n",
      "1 individual\n",
      "1 individuals\n",
      "1 indwelling\n",
      "1 inflammation\n",
      "2 inflammatory\n",
      "1 influence\n",
      "1 inform\n",
      "1 initiative\n",
      "1 inpatient\n",
      "1 institution\n",
      "1 insurance\n",
      "1 integral\n",
      "1 integrated\n",
      "1 integumentary\n",
      "2 interest\n",
      "1 interindividual\n",
      "1 interplay\n",
      "1 interquartile\n",
      "4 interval\n",
      "1 intervals\n",
      "2 intervention\n",
      "1 intrasurvey\n",
      "1 investigated\n",
      "1 involved\n",
      "1 involving\n",
      "2 iqr\n",
      "1 january\n",
      "1 july\n",
      "1 june\n",
      "1 kendall\n",
      "1 key\n",
      "1 km\n",
      "2 known\n",
      "1 lack\n",
      "1 large\n",
      "3 lasso\n",
      "1 lastly\n",
      "1 later\n",
      "1 leads\n",
      "8 learning\n",
      "1 least\n",
      "1 led\n",
      "1 length\n",
      "1 less\n",
      "1 level\n",
      "4 likelihood\n",
      "1 likelihoods\n",
      "2 likely\n",
      "1 limiting\n",
      "1 linear\n",
      "1 link\n",
      "2 little\n",
      "4 location\n",
      "2 logistic\n",
      "1 los\n",
      "1 love\n",
      "4 lower\n",
      "1 lowest\n",
      "9 machine\n",
      "1 making\n",
      "3 male\n",
      "1 males\n",
      "2 management\n",
      "2 manchester\n",
      "1 manila\n",
      "1 mantel\n",
      "1 materials\n",
      "7 maximum\n",
      "7 may\n",
      "1 mean\n",
      "1 meanings\n",
      "3 measured\n",
      "1 measurements\n",
      "3 measures\n",
      "2 mechanisms\n",
      "2 media\n",
      "9 median\n",
      "3 medical\n",
      "7 medicine\n",
      "1 merely\n",
      "1 merit\n",
      "1 met\n",
      "3 method\n",
      "3 methods\n",
      "1 metropolitan\n",
      "1 mild\n",
      "2 million\n",
      "1 min\n",
      "1 minimal\n",
      "2 minutes\n",
      "2 mixed\n",
      "3 model\n",
      "2 models\n",
      "2 moderate\n",
      "2 modern\n",
      "1 moran\n",
      "2 multidisciplinary\n",
      "2 multimodal\n",
      "2 multiple\n",
      "1 musculoskeletal\n",
      "2 must\n",
      "1 naivebayesupdateable\n",
      "2 national\n",
      "1 nearest\n",
      "2 necessary\n",
      "1 need\n",
      "1 neighbor\n",
      "10 nerve\n",
      "1 nervous\n",
      "1 network\n",
      "3 networks\n",
      "1 neural\n",
      "1 next\n",
      "2 nn\n",
      "1 nociception\n",
      "1 node\n",
      "1 nomenclature\n",
      "1 nominated\n",
      "3 non\n",
      "2 nonambulatory\n",
      "1 nonobstetric\n",
      "1 nonzero\n",
      "1 not\n",
      "1 novel\n",
      "5 nrs\n",
      "2 number\n",
      "2 numeric\n",
      "2 numerical\n",
      "1 nursing\n",
      "2 obama\n",
      "2 objective\n",
      "1 observation\n",
      "2 observations\n",
      "1 observed\n",
      "1 obstetric\n",
      "1 obtained\n",
      "1 occurring\n",
      "4 odds\n",
      "1 offer\n",
      "1 often\n",
      "6 older\n",
      "1 one\n",
      "1 online\n",
      "1 open\n",
      "3 operating\n",
      "1 operator\n",
      "1 optimal\n",
      "2 optimized\n",
      "4 or\n",
      "1 organization\n",
      "1 organizational\n",
      "5 orthopedic\n",
      "5 outcome\n",
      "6 outcomes\n",
      "3 overall\n",
      "7 pacu\n",
      "91 pain\n",
      "1 painful\n",
      "3 panel\n",
      "1 party\n",
      "1 pathophysiologic\n",
      "1 pathways\n",
      "8 patient\n",
      "23 patients\n",
      "1 pattern\n",
      "1 patterns\n",
      "1 pay\n",
      "1 payers\n",
      "3 payment\n",
      "1 peak\n",
      "2 peaked\n",
      "4 pepm\n",
      "4 per\n",
      "1 percentage\n",
      "2 perceptions\n",
      "4 performance\n",
      "2 performed\n",
      "1 performing\n",
      "4 period\n",
      "1 periods\n",
      "4 perioperative\n",
      "2 peripheral\n",
      "3 pertaining\n",
      "2 phase\n",
      "1 physician\n",
      "3 placement\n",
      "8 pod\n",
      "7 pods\n",
      "4 points\n",
      "2 policy\n",
      "1 popularity\n",
      "2 population\n",
      "3 positive\n",
      "2 post\n",
      "1 posted\n",
      "18 postoperative\n",
      "1 posts\n",
      "6 practice\n",
      "1 practices\n",
      "4 pre\n",
      "3 predict\n",
      "1 predicted\n",
      "2 predicting\n",
      "1 prediction\n",
      "1 predictive\n",
      "4 preoperative\n",
      "1 presence\n",
      "1 preserved\n",
      "1 pressor\n",
      "1 prevalence\n",
      "4 primary\n",
      "4 prior\n",
      "1 priorities\n",
      "1 private\n",
      "1 pro\n",
      "2 procedure\n",
      "5 procedures\n",
      "2 process\n",
      "2 processing\n",
      "1 produced\n",
      "1 professional\n",
      "1 profile\n",
      "1 program\n",
      "3 programs\n",
      "2 project\n",
      "1 prolonged\n",
      "1 prompt\n",
      "3 proportion\n",
      "1 proposes\n",
      "1 providers\n",
      "1 provision\n",
      "1 published\n",
      "1 purpose\n",
      "1 quality\n",
      "1 range\n",
      "4 ranged\n",
      "2 ranging\n",
      "1 rapid\n",
      "1 rates\n",
      "4 rating\n",
      "3 ratio\n",
      "2 receive\n",
      "3 receiver\n",
      "4 receiving\n",
      "1 recent\n",
      "1 recently\n",
      "1 recognition\n",
      "1 recognized\n",
      "1 record\n",
      "6 recorded\n",
      "3 records\n",
      "3 recovery\n",
      "1 reduce\n",
      "1 reduced\n",
      "3 reduction\n",
      "1 reflect\n",
      "1 reflecting\n",
      "1 regard\n",
      "6 regional\n",
      "1 registry\n",
      "2 regression\n",
      "8 related\n",
      "1 relief\n",
      "1 rely\n",
      "1 remains\n",
      "5 report\n",
      "2 reported\n",
      "1 reports\n",
      "1 repository\n",
      "1 represent\n",
      "1 represents\n",
      "3 request\n",
      "1 requests\n",
      "1 require\n",
      "3 required\n",
      "3 requirements\n",
      "1 requiring\n",
      "2 research\n",
      "1 resident\n",
      "1 residents\n",
      "1 resource\n",
      "1 response\n",
      "3 responses\n",
      "1 result\n",
      "6 results\n",
      "6 retrospective\n",
      "2 review\n",
      "5 reviewed\n",
      "3 risk\n",
      "5 roc\n",
      "3 role\n",
      "1 sad\n",
      "2 sample\n",
      "1 satisfaction\n",
      "4 scale\n",
      "2 scope\n",
      "8 score\n",
      "28 scores\n",
      "1 seasonal\n",
      "1 second\n",
      "1 secondary\n",
      "1 seconds\n",
      "1 sectional\n",
      "1 selection\n",
      "1 self\n",
      "2 sentiment\n",
      "2 sequential\n",
      "4 service\n",
      "1 services\n",
      "1 sessions\n",
      "6 set\n",
      "4 setting\n",
      "4 severe\n",
      "5 sex\n",
      "1 shared\n",
      "1 show\n",
      "1 shrinkage\n",
      "2 significant\n",
      "2 significantly\n",
      "1 similar\n",
      "1 simple\n",
      "3 single\n",
      "1 slightly\n",
      "4 social\n",
      "1 society\n",
      "2 socioeconomic\n",
      "1 solution\n",
      "1 source\n",
      "1 sparsity\n",
      "6 spe\n",
      "1 special\n",
      "1 specifically\n",
      "1 spes\n",
      "2 staffing\n",
      "1 stake\n",
      "1 standards\n",
      "4 states\n",
      "1 statistical\n",
      "1 statistically\n",
      "1 statistics\n",
      "1 step\n",
      "1 stimuli\n",
      "1 strategy\n",
      "2 stratification\n",
      "1 strong\n",
      "1 strongly\n",
      "1 structures\n",
      "2 studies\n",
      "13 study\n",
      "2 subacute\n",
      "4 subjects\n",
      "1 subsequent\n",
      "2 subspecialty\n",
      "1 successful\n",
      "3 suggest\n",
      "1 summary\n",
      "3 support\n",
      "1 supported\n",
      "20 suppr\n",
      "1 surge\n",
      "25 surgery\n",
      "14 surgical\n",
      "2 survey\n",
      "1 sustained\n",
      "1 symptom\n",
      "1 synthesized\n",
      "2 system\n",
      "4 systems\n",
      "1 take\n",
      "2 taken\n",
      "1 targeted\n",
      "1 task\n",
      "2 tau\n",
      "2 teaching\n",
      "1 teams\n",
      "2 techniques\n",
      "1 temporally\n",
      "1 term\n",
      "5 terms\n",
      "2 tertiary\n",
      "2 test\n",
      "3 tested\n",
      "1 theory\n",
      "3 therapies\n",
      "1 thermal\n",
      "1 third\n",
      "1 this\n",
      "1 three\n",
      "2 throughout\n",
      "15 time\n",
      "3 times\n",
      "1 tired\n",
      "2 tnf\n",
      "2 together\n",
      "1 tool\n",
      "1 tools\n",
      "4 total\n",
      "1 toward\n",
      "1 traditional\n",
      "3 training\n",
      "4 trajectories\n",
      "1 trajectory\n",
      "1 trauma\n",
      "1 treating\n",
      "2 tree\n",
      "2 tweeters\n",
      "5 tweets\n",
      "1 twitter\n",
      "2 two\n",
      "7 type\n",
      "1 ultimate\n",
      "1 unavailable\n",
      "1 unclear\n",
      "1 undergo\n",
      "3 undergoing\n",
      "1 understanding\n",
      "1 underwent\n",
      "1 undesirable\n",
      "1 uniform\n",
      "1 uninterrupted\n",
      "1 unique\n",
      "2 unit\n",
      "6 united\n",
      "3 units\n",
      "1 university\n",
      "1 unnecessary\n",
      "1 unobtrusively\n",
      "1 use\n",
      "1 used\n",
      "1 users\n",
      "15 using\n",
      "1 utilizing\n",
      "1 validated\n",
      "3 variability\n",
      "5 variables\n",
      "1 variety\n",
      "1 vector\n",
      "1 versus\n",
      "1 vexing\n",
      "1 vital\n",
      "2 vs\n",
      "1 warrants\n",
      "1 weighted\n",
      "8 well\n",
      "1 whether\n",
      "1 which\n",
      "1 widespread\n",
      "4 within\n",
      "1 without\n",
      "1 witnessed\n",
      "1 word\n",
      "4 work\n",
      "1 would\n",
      "5 year\n",
      "1 years\n",
      "1 yield\n",
      "2 yielded\n",
      "3 younger\n"
     ]
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13x796 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1225 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another, slightly more robust approach from http://blog.yhat.com/posts/predict-weather-with-kaggle-twitter-emoticons-pandas.html\n",
    "\n",
    "ca = papers['cleaned_abstract'].tolist()\n",
    "vectorizer = CountVectorizer(\n",
    "                             analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "vectorizer.fit(ca)\n",
    "x = vectorizer.transform(ca)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_sentences(filename):\n",
    "    \"\"\"\n",
    "        Breaks abstract into sentences\n",
    "        \"\"\"\n",
    "\n",
    "    print \"\\nTokenizing abstract\\n\\n\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    fp = open(filename)\n",
    "    data = fp.read()\n",
    "    return tokenizer.tokenize(data.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word2vec_sentence(sentence_list, stoplist):\n",
    "    print \"\\nCreating word2vec sentences\\n\\n\"\n",
    "    retList = list()\n",
    "    toolbar_width = len(sentence_list)\n",
    "    for i, sentence in enumerate(sentence_list):\n",
    "\n",
    "        p = str((float(i+1)/toolbar_width)*100)[:4]\n",
    "        sys.stdout.write(\"\\r%s%%\" %p)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        sentLst = [x.strip(\",\").rstrip(\".\").strip(\":\").lower() for x in sentence.split(\" \") if x.strip(\",\").strip(\".\").strip(\":\").lower() not in stoplist]\n",
    "        retList.append(sentLst)\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Can try this later\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "papers['stemab_p']= papers['cleaned_abstract'].str.split()\n",
    "papers['stemab']= papers['cleaned_abstract'].map(lambda x: lemmer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
