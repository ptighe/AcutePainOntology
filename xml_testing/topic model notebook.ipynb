{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load topic_model.py\n",
    "\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from Bio import Entrez\n",
    "from Bio.Entrez import efetch, read\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "\n",
    "import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_abstract(abstract_data):\n",
    "    \n",
    "    abstract_data = abstract_data.split(\"\\n\")\n",
    "    Flag = False\n",
    "    parserFlag = False\n",
    "    abstract_lines = list()\n",
    "    \n",
    "    for lines in abstract_data:\n",
    "        if \"Author information:\" in lines:\n",
    "            Flag = True\n",
    "        \n",
    "        if Flag == True and lines == \"\":\n",
    "            parserFlag = True\n",
    "            Flag = False\n",
    "            continue\n",
    "        \n",
    "        if Flag == False and lines == \"\":\n",
    "            parserFlag = False\n",
    "        \n",
    "        if parserFlag == True:\n",
    "#             abstract_lines.append(lines.strip().decode('utf-8'))\n",
    "            abstract_lines.append(lines.strip())\n",
    "\n",
    "    return \" \".join(abstract_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_abstract(pmid):\n",
    "    handle = efetch(db='pubmed', id=pmid, retmode='text', rettype='abstract')\n",
    "    data = handle.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_sentences(filename):\n",
    "    \"\"\"\n",
    "        Breaks abstract into sentences\n",
    "        \"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    fp = open(filename)\n",
    "    data = fp.read()\n",
    "#     return tokenizer.tokenize(data.decode('utf-8'))\n",
    "    return tokenizer.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_abstracts(keyword, n):\n",
    "    link_keyword = \"+\".join(keyword.strip().split(\" \"))\n",
    "    exec_string = \"curl -vs 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=\" + link_keyword + \"&retmode=text&retmax=\" + str(n) + \"' 2>&1 | grep '^<Id>' > pmid.txt\"\n",
    "    os.system(exec_string)\n",
    "\n",
    "    fileObj = open(\"pmid.txt\", \"r\")\n",
    "    idList = list()\n",
    "    for lines in fileObj:\n",
    "        idList.append(lines[4:12])\n",
    "        \n",
    "    fq = open(\"abstracts.txt\", \"w\")\n",
    "    toolbar_width = len(idList)\n",
    "\n",
    "    print (\"Total PMIDs =\", toolbar_width)\n",
    "\n",
    "    for i, pmid in enumerate(idList):\n",
    "        p = str((float(i+1)/toolbar_width)*100)[:4]\n",
    "        sys.stdout.write(\"\\r%s%%\" %p)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        abstract_para = clean_abstract(fetch_abstract(pmid))\n",
    "#         fq.write(abstract_para.encode('utf-8'))\n",
    "        fq.write(abstract_para)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    fq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = [\"the\", \"in\", \"it\", \"on\", \"and\", \"was\", \"group\", \"ci\", \"of\", \"to\", \"that\", \"a\", \"were\", \"by\", \"il\", \"to\", \"this\", \"is\", \"for\", \"has\", \"been\", \"are\", \"with\", \"or\", \"an\", \"had\", \"has\", \"be\", \"they\", \"them\", \"as\", \"at\", \"we\", \"there\", \"from\", \"who\", \"not\", \"=\", \"no\", \"methods:\", \"results:\", \"than\", \"all\", \"vs.\", \"Â±\", \"he\", \"she\", \"(p\", \"but\", \"their\", \"our\", \"but\", \"also\", \"can\", \"conclusions:\", \"two\", \"due\", \"only\", \"did\", \"one\", \"used\", \"may\", \"these\", \"both\", \"data\", \"have\", \"other\", \"any\", \"i.t\", \"1\"]\n",
    "\n",
    "Entrez.email = 'ptighe@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PMIDs = 100\n",
      "100.%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pull_abstracts(\"Pain\", 100) # first element is the keyword and second element is the maximum number of abstract to be pulled\n",
    "documents = pull_sentences('abstracts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"pain_sentences.txt\", \"w\") as fp:\n",
    "    for i, sentence in enumerate(documents):\n",
    "        fp.write(\"%s\\n\" %sentence)\n",
    "        documents[i] = sentence\n",
    "        if documents[i][-1] == \".\":\n",
    "            documents[i] = documents[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-270b96c74f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "w_model = Word2Vec(documents, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "dictionary.save('pain.dict')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('pain.mm', corpus)\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating LSI Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "LSI Model Details\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Creating LDA Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "LDA Model Details\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LSI model\n",
    "\n",
    "print(\"\\n\\nCreating LSI Model\\n\\n\")\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\\nLSI Model Details\\n\\n\")\n",
    "lsi.print_topics(100)\n",
    "print(\"--------------------------------------------------------------------------------------------------------------------------\\n\\n\\n\")\n",
    "lsi.save('acutepain_LSImodel.lsi')\n",
    "\n",
    "# LDA model\n",
    "print(\"\\n\\nCreating LDA Model\\n\\n\")\n",
    "model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=5)\n",
    "print(\"\\n\\n--------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\\nLDA Model Details\\n\\n\")\n",
    "model.print_topics(100)\n",
    "print(\"--------------------------------------------------------------------------------------------------------------------------\")\n",
    "model.save('acutepain_LDAmodel.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
